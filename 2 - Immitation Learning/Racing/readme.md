# Immitation Learning for Car Racing

## Problem Statement

You have to play Open AI Gym's CarRacing game to produce state images and their corresponding expert action. Then train an agent to mimic this expert behaviour.

1. Familiar yourself with the CarRacing environment by running drive_manually.py in the starter code repository. Car controls are keys -> up, down, left, right. Please go through the code in the repository and see how they are interfacing with each other.
2. Collect the driving data with `python drive_manually.py --collect_data`. It will be stored in the /data/ folder
3. Use the collected data to train the behavioral cloning agent. Implement the code in the train_agent.py file.
4. Evaluate the agent by making it play the game. Implement the code in the test_agent.py file.
5. Improve your agent and explain what helped.
6. Plot the performance of your model as a function of the training data.


## My Solution

**Final result - My agent playing the game:**

![My Trained Agent](https://github.com/SohamTamba/DeepRL/tree/master/2%20-%20Immitation%20Learning/Racing/racing-demo.gif)

I played the game to generate 25,000 state/action pairs, 23,000 of which formed the training dataset and the remainder formed the validation dataset. I achieved an average score of approximate 800, after practicing for a few hours.

The architecture of my agent model can be decomposed into a encoder and decoder:

**Agent model architecture:**

![Agent Model Architecture](https://github.com/SohamTamba/DeepRL/tree/master/2%20-%20Immitation%20Learning/Racing/model.png)

* Encoder: A ResNet-18 pretrained on ImageNet.
* Decoder: A stack of fully connected layers, with a Batchnorm and ReLU in between 2 consecutive layers.


My agent accepts the last N state images as input and infers the expert action. Invalid images were zero-ed out. For example, if N=4, and the agent has only seen the first image X, then state-img-1 = X and state-img-2 = state-img-3 = state-img-4 = torch.zeros(image_dimensions)

I tried solving the problem as a regression task as well as a classification task.

1. Regression: The model outputs the acceleration and steering of the car. It is worth noting that in this prediction mode, the model can output an action that cannot be generated by a combination of car controls. Mean Square Error Loss is employed.
2. Classification: The model outputs the control key to be pressed (up, down, left, right, none) and infers the action from it. Cross Entropy Loss is employed.

Hyper-parameters:

* Learning Rate: 0.0001
* Optimizer: SGD with momentum of 0.9, and learning rate annealed by a factor 0.1 when the validation loss does not decrease for 4 epochs
* History (N): 8
* Number of Epochs: 60
* Batch Size: 32
* Training Dataset Size: 23000
* Validation Dataset Size: 2000

### Results

I studied the effect of:

1. Fine-tuning the encoder vs freezing it
2. Varrying the depth of the decoder
3. Regressing vs Classifying the expert action

**Performance of the models:**

![Performance of the models](https://github.com/SohamTamba/DeepRL/tree/master/2%20-%20Immitation%20Learning/Racing/performance.png)

The following adjustments improved performance:

1. Reducing the initial learning rate from the default 0.01 to 0.0001: The default learning rate would cause training to diverge and loss to explode in the first epoch.
2. Using 2 fully connected layers in the decoder: Using 3 or 1 layers would increase the validation loss, probably due to overfitting and underfitting respectively.
3. Training the encoder: Since the encoder was pretrained on ImageNet, whose images are of a very different domain. For example, ImageNet images have much higher frequency than the smooth artificially generated images generated by the gym. So allowing the encoder to train on these images improved its performance.
4. Classifying the action: Using the classification training mode allowed
us to inject the inductive bias into the model regarding the effects of pressing a single key (or pressing none).

I studied the effect of training the best performing model on a fraction of the data:

![Agent reward with varrying training data](https://github.com/SohamTamba/DeepRL/tree/master/2%20-%20Immitation%20Learning/Racing/reward-vs-data.png)

It appears that the size of the training data has little effect on the performance. In fact the best performing agent scores an **average reward of 801**, using only 40% of the data.


The tensorboard files ploting the training curves can be found in the /train_outputs/ folder.

## Run The Code

To play the game and collect data, execute:

`python drive_manually.py --collect_data`

To train the model, execute:

`python train_agent.py -num_decoder_layers 2 -prediction_mode classification --download_encoder --save_output -batch_size 32`

To evaluate the model, execute:

`python test_agent.py -num_decoder_layers 2 -prediction_mode classification --render`

